{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pipelines and custom transfomers in SKLearn\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Pipelines and custom transfomers in SKLearn\n",
    "Week 5 | Lesson 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- create pipelines for cleaning and manipulating data\n",
    "- use pipelines to preprocess data from the SQL database\n",
    "- use pipeline in combination with classification\n",
    "- create a custom transformer using the `TransformerMixin` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- extract data from a database\n",
    "- perform classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTOR PREP\n",
    "*Before this lesson, instructors will need to:*\n",
    "- Read in / Review any dataset(s) & starter/solution code\n",
    "- Generate a brief slide deck\n",
    "- Prepare any specific materials\n",
    "- Provide students with additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LESSON GUIDE\n",
    "\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 mins | [Opening](#opening) | Opening |\n",
    "| 10 mins | [Introduction](#introduction) | Data Pipelines |\n",
    "| 25 mins | [Demo](#demo) | Pipelines in scikit-learn |\n",
    "| 15 mins | [Guided-practice](#guided-practice) | Make Pipeline and the preprocessing module |\n",
    "| 10 minutes | [Demo](#demo_2) | Custom Transformers |\n",
    "| 20 minutes | [Ind-practice](#ind-practice) | Putting it all together |\n",
    "| 5 mins | [Conclusion](#conclusion) | Conclusion |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 mins)\n",
    "Many organizations rely on data engineering teams to encode these common tasks into pipelines. **Data pipelines** are a series of automated data transformations that ensure the validity of your work for routine data maintenance tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Data Pipelines (10 mins)\n",
    "\n",
    "The term _Pipeline_ is used to indicate a series of concatenated data transformations. Each stage of the pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end as water flows through... a pipeline.\n",
    "\n",
    "![Pipeline](./assets/images/pipeline.png)\n",
    "\n",
    "Each processing stage has an input, where data comes in, and an output, where processed data comes out.\n",
    "\n",
    "**Check:** Ask the students to give some examples of data transformations.\n",
    "\n",
    "> Examples of data transformations:\n",
    "> - change in units (lbs -> kg)\n",
    "> - change of scale\n",
    "> - change of base\n",
    "> - text vectorization\n",
    "> - image vectorization\n",
    "> - sound file vectorization\n",
    "> - missing data imputation\n",
    "> - clipping\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Pipelines in scikit-learn (25 mins)\n",
    "One way to improve coding and model management is to use pipelines in `scikit-learn`. These tie together all the steps that you may need to prepare your dataset and make your predictions. Because you will need to perform all of the exact same transformations on your evaluation data, encoding the exact steps is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how a pipeline works, we'll use an example involving Natural Language Processing. Data comes from the [Evergreen Stumbleupon Kaggle Competition](https://www.kaggle.com/c/stumbleupon/data), where participants where challenged to build a classifier to categorize webpages as evergreen or non-evergreen. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) were provided. We'll focus on the page title text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1    The Fully Electronic Futuristic Starting Gun T...\n",
       "2    Fruits that Fight the Flu fruits that fight th...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"./assets/datasets/stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "titles = data['title'].fillna('')\n",
    "titles[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['label']\n",
    "y[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51332\n",
       "0    0.48668\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each datapoint is a string of free form text. How can we feed this to a model? The simplest way is to build a dictionary of words and use those as features. This is what a `CountVectorizer` does.\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "|Sentence|the|cat|is|on|table|blue|\n",
    "|---|---|---|---|---|---|---|\n",
    "|The cat is on the table|2|1|1|1|1|0|\n",
    "|The table is blue|1|0|1|0|1|1|\n",
    "|...||||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=(1, 2),\n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "vectorizer.fit(['IBM Sees Holographic Calls Air Breathing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'air',\n",
       " u'air breathing',\n",
       " u'breathing',\n",
       " u'calls',\n",
       " u'calls air',\n",
       " u'holographic',\n",
       " u'holographic calls',\n",
       " u'ibm',\n",
       " u'ibm sees',\n",
       " u'sees',\n",
       " u'sees holographic']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['IBM Sees Holographic Air']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check**What is the meaning of the various parameters used at initialization of the Vectorizer?\n",
    "\n",
    "\n",
    "Let's use the vectorizer to fit all the titles and build a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used this input X, a matrix of all common n-grams in the dataset, as an input to our classifier. We wanted to classify how evergreen a story was, based on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [ 0.74574209  0.75659229  0.75487013]\n",
      "Average CVScore: 0.752 +/- 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model, X, y)\n",
    "\n",
    "print('CV scores: {}'.format(scores))\n",
    "\n",
    "print('Average CVScore: {:0.3f} +/- {:0.3f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Steps in Pipelines\n",
    "\n",
    "Often we will want to combine these steps to evaluate some future dataset. Therefore, we need to make sure we perform the _exact same_ transformations on the data. If \"has_brownies_in_text\" is column 19, we need to make sure it is __also__ column 19 during future evaluation.\n",
    "\n",
    "Pipelines combines both pre-processing and model building steps into a _single object_. Rather than manually evaluating the transformers and then feeding them into the models, pipelines ties both of these steps together.\n",
    "\n",
    "Similar to models and vectorizers in scikit-learn, pipelines are equipped with `fit` and `predict` or `predict_proba` methods (as any model would be), but they ensure that proper data transformations are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46800424,  0.53199576],\n",
       "       [ 0.28316267,  0.71683733],\n",
       "       [ 0.00514   ,  0.99486   ],\n",
       "       ..., \n",
       "       [ 0.29063018,  0.70936982],\n",
       "       [ 0.60683954,  0.39316046],\n",
       "       [ 0.66318704,  0.33681296]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into a training set\n",
    "training_data = data[:6000]\n",
    "X_train = training_data['title'].fillna('')\n",
    "y_train = training_data['label']\n",
    "\n",
    "# These rows are rows obtained in the future, unavailable at training time\n",
    "X_new = data[6000:]['title'].fillna('')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vec', vectorizer),\n",
    "        ('model', model)   \n",
    "    ])\n",
    "\n",
    "# Fit the full pipeline\n",
    "# This means we perform the steps laid out above\n",
    "# First we fit the vectorizer,\n",
    "# And then feed the output of that into the fit function of the model\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Here again we apply the full pipeline for predictions\n",
    "# The text is transformed automatically to match the features from the pipeline\n",
    "pipeline.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Add a `MaxAbsScaler` scaling step to the pipeline, which should occur after the vectorization.\n",
    "\n",
    "#### Merging Feature Sets in Pipelines\n",
    "\n",
    "Additionally, we want to merge many different feature sets automatically. Here we can use scikit-learn's `FeatureUnion`.\n",
    "\n",
    "While scikit-learn pipelines help with managing the transformation from raw data, there may be many steps before this takes place in your pipeline. These pipelines are often referred to as _ETL pipelines_ for \"Extract, Transform, Load.\"\"\n",
    "\n",
    "In an _ETL pipeline_, the data is pulled or extracted from some source (like a database), transformed or manipulated, and then \"loaded\" into whatever system or analysis requires them.\n",
    "\n",
    "Many data science teams rely on software tools to manage these ETL pipelines. If a transformation step fails, these tools alert you, or ensure that step can be re-run. If these transformation steps need to happen daily or weekly, these tools can manage that timeline.\n",
    "\n",
    "- One of the most popular Python tools for this is [Luigi](https://github.com/spotify/luigi) developed by Spotify.\n",
    "- An alternative is [Airflow](https://github.com/airbnb/airflow) by AirBnB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Make Pipeline and the preprocessing module (15 mins)\n",
    "\n",
    "#### make_pipeline\n",
    "Scikit-learn pipelines can also be built using the `make_pipeline` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), LogisticRegression())    \n",
    "\n",
    "pipe2 = Pipeline(steps=[('standardscaler',StandardScaler()),\n",
    "                        ('logistic_regr',LogisticRegression())\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logistic_regr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two pipelines created above are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing in sklearn (in pairs)\n",
    "\n",
    "The preprocessing module comes loaded with many very useful pre-processing classes.\n",
    "\n",
    "**Check** in pairs, assign one function to each pair, they have to read about it in the doc and then explain it to the class.\n",
    "\n",
    "\n",
    "Data Manipulators\n",
    "- Binarizer\n",
    "- KernelCenterer\n",
    "- MaxAbsScaler\n",
    "- MinMaxScaler\n",
    "- Normalizer\n",
    "- OneHotEncoder\n",
    "- PolynomialFeatures\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "Data Imputation\n",
    "- Imputer\n",
    "\n",
    "Function Transformer\n",
    "- FunctionTransformer\n",
    "\n",
    "Label Manipulators\n",
    "- LabelBinarizer\n",
    "- LabelEncoder\n",
    "- MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo_2\"></a>\n",
    "## Custom Transformers (10 minutes)\n",
    "\n",
    "We can implement custom transformers by extending the BaseClass in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 0, 6, 0],\n",
       "       [0, 0, 0, 8]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        return X * self.factor\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "fm = FeatureMultiplier(2)\n",
    "\n",
    "test = np.diag((1,2,3,4))\n",
    "print test\n",
    "\n",
    "fm.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Compare this with the `FunctionTransformer` from the preprocessing module.\n",
    "\n",
    "**Check** Implement a custom transformer that selects a specific feature from a Pandas dataframe. It should be initialized with the column name or the column index and it should return the selected column when transforming a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Putting it all together (20 minutes)\n",
    "\n",
    "**Check** Revisit the dataset of lab 1.4. How could you use `make_pipeline` and `make_union` to build a pipeline that performs the same steps all in one pass?\n",
    "\n",
    "> Answer: you will have to build something like this:\n",
    ">\n",
    "    Data --> SelectCategoricalFeaturesTransformer --> OneHotEncoder --> FeatureUnion --> Model\n",
    "          \\-> SelectNumericalFeaturesTransformer ------> Scaler ----/\n",
    "A good practice for instructor is to have students work in this way:\n",
    "1. review lab 1.4 and identify the steps that were perfomed\n",
    "- for each of this steps figure out what the input and what the output is\n",
    "    - is the input the whole dataframe or only a subset of the features?\n",
    "    - is the output new features or a prediction?\n",
    "- for each of this steps idendify what kind of transformer is needed:\n",
    "    - is it a custom transformer?\n",
    "    - does scikit-learn provide a transformer like this out of the box?\n",
    "- if different features are treated differently, have students figure out how to recombine them (Feature Union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "We have learnt how to use the `Pipeline` construct in order to chain several instructions in one single class. This enables to treat data-processing from a more abstract and more powerful perspective, and it's a pre-cursor to the work we will do when working with Big Data technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Pipelines and Feature Union](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "- [Example with complex pipeline](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#example-hetero-feature-union-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
