{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines and custom transformers in sklearn\n",
    "\n",
    "---\n",
    "\n",
    "**Data pipelines** are a series of automated data transformations used to perform (and ensure the validity of) routine data maintenance and analysis tasks. \n",
    "\n",
    "Many organizations rely on data engineering teams to encode common tasks into pipelines. It is likely that you will at some point be required to productionize a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "The term **pipeline** is jargon for **a series of concatenated data transformations**. Each stage of a pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"./assets/images/pipeline.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks of a data science process and are a great way to organize analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "What are some examples of data pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Change in units (lbs -> kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Change in scale (normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Missing data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Image & sound processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines in scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "Pipelines improve coding and model management in ```scikit-learn```. These tie together all the steps that you may need to prepare your datasets and make your predictions. \n",
    "\n",
    "Because you will need to perform all of the exact same transformations on your evaluation data, encoding the exact steps is important for reproducibility and consistency. **This is especially important and convenient when sharing code with a team!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Loading the sklearn pipeline code:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example with natural language processing (NLP)\n",
    "\n",
    "---\n",
    "\n",
    "**NLP** is currently a very popular and high demand skill in the world of data science. It is essentially statistics and machine learning applied to human text data. A very common example of this is \"comments\", which are now common in nearly every app and website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "Our practice data comes from the **Evergreen Stumbleupon Kaggle Competition**. Participants where challenged to build a classifier to categorize webpages as \"evergreen\" or \"non-evergreen\".\n",
    "\n",
    "Check out the information on Kaggle here:\n",
    "\n",
    "    https://www.kaggle.com/c/stumbleupon/data\n",
    "    \n",
    "Binary evergreen labels (either evergreen (1) or non-evergreen (0)) were provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load the local Kaggle dataset:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>urlid</th>\n",
       "      <th>boilerplate</th>\n",
       "      <th>alchemy_category</th>\n",
       "      <th>alchemy_category_score</th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>...</th>\n",
       "      <th>is_news</th>\n",
       "      <th>lengthyLinkDomain</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>news_front_page</th>\n",
       "      <th>non_markup_alphanum_characters</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>parametrizedLinkRatio</th>\n",
       "      <th>spelling_errors_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bloomberg.com/news/2010-12-23/ibm-p...</td>\n",
       "      <td>4042</td>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "      <td>business</td>\n",
       "      <td>0.789131</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>5424</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.07913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  urlid  \\\n",
       "0  http://www.bloomberg.com/news/2010-12-23/ibm-p...   4042   \n",
       "\n",
       "                                         boilerplate alchemy_category  \\\n",
       "0  {\"title\":\"IBM Sees Holographic Calls Air Breat...         business   \n",
       "\n",
       "  alchemy_category_score  avglinksize  commonlinkratio_1  commonlinkratio_2  \\\n",
       "0               0.789131     2.055556           0.676471           0.205882   \n",
       "\n",
       "   commonlinkratio_3  commonlinkratio_4  ...    is_news  lengthyLinkDomain  \\\n",
       "0           0.047059           0.023529  ...          1                  1   \n",
       "\n",
       "   linkwordscore  news_front_page  non_markup_alphanum_characters  \\\n",
       "0             24                0                            5424   \n",
       "\n",
       "   numberOfLinks  numwords_in_url parametrizedLinkRatio  \\\n",
       "0            170                8              0.152941   \n",
       "\n",
       "   spelling_errors_ratio  label  \n",
       "0                0.07913      0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this we are going to use the json package as well to create the \"title\" \n",
    "# and \"body\" columns from the \"boilerplate\" column, which is in json format\n",
    "import json\n",
    "\n",
    "# it is a tab delimited file:\n",
    "data = pd.read_csv(\"./assets/datasets/stumbleupon.tsv\", sep='\\t')\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1    The Fully Electronic Futuristic Starting Gun T...\n",
       "2    Fruits that Fight the Flu fruits that fight th...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the columns by converting json:\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "# change title and body na columns to blank strings\n",
    "titles = data['title'].fillna('')\n",
    "body = data['body'].fillna('')\n",
    "\n",
    "titles[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Make and check the target for the classifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data['label']\n",
    "Y[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51332\n",
       "0    0.48668\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CountVectorizer\n",
    "\n",
    "---\n",
    "\n",
    "Each datapoint is a string of free form text. How can we make this numeric and useful for a classification model? \n",
    "\n",
    "One of the simplest and most common ways is to **build a dictionary of words and use those as features**. \n",
    "\n",
    "This is what sklearns **`CountVectorizer`** does.\n",
    "\n",
    "_Example output:_\n",
    "\n",
    "\n",
    "|Sentence|the|cat|is|on|table|blue|\n",
    "|---|---|---|---|---|---|---|\n",
    "|The cat is on the table|2|1|1|1|1|0|\n",
    "|The table is blue|1|0|1|0|1|1|\n",
    "|...|...|...|...|...|...|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load and apply the CountVectorizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'air',\n",
       " u'air breathing',\n",
       " u'breathing',\n",
       " u'calls',\n",
       " u'calls air',\n",
       " u'holographic',\n",
       " u'holographic calls',\n",
       " u'ibm',\n",
       " u'ibm sees',\n",
       " u'sees',\n",
       " u'sees holographic']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We want 1000 words as features max.\n",
    "# ngram_range = (1,2) specifies that we want single words and word pairs\n",
    "# stop_words = 'english' specifies that we do not want the most common english words\n",
    "# binary = True means we want 1 if the word was present rather than a count of that word\n",
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=(1, 2),\n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "# fit on an example:\n",
    "vectorizer.fit(['IBM Sees Holographic Calls Air Breathing'])\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example row from Countvectorizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['IBM Sees Holographic Air']).todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create a vectorizer for the title and body columns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "title_vectorizer = CountVectorizer(max_features = 1000,\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   stop_words='english',\n",
    "                                   binary=True)\n",
    "\n",
    "body_vectorizer = CountVectorizer(max_features = 1000,\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  stop_words='english',\n",
    "                                  binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "title_vectorizer.fit(titles)\n",
    "\n",
    "# and the vocabulary of the body\n",
    "body_vectorizer.fit(body)\n",
    "\n",
    "# Use `transform` to generate the sample title and body word matrix\n",
    "# one column per feature (word or n-grams)\n",
    "title_X = title_vectorizer.transform(titles)\n",
    "body_X = body_vectorizer.transform(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use **`title_X`** (a matrix of all common title n-grams in the dataset), as an input to a logistic regression classifier. \n",
    "\n",
    "**Classify how whether a story is evergreen based on title features:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [ 0.75743243  0.75997295  0.75794456  0.74983097  0.76589986]\n",
      "Average CVScore: 0.758 +/- 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "title_model = LogisticRegression()\n",
    "title_scores = cross_val_score(title_model, title_X, Y, cv=5)\n",
    "\n",
    "print('CV scores: {}'.format(title_scores))\n",
    "\n",
    "print('Average CVScore: {:0.3f} +/- {:0.3f}'.format(title_scores.mean(), title_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Try on the body features:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [ 0.73243243  0.74104124  0.73427992  0.74171738  0.7435724 ]\n",
      "Average CVScore: 0.739 +/- 0.004\n"
     ]
    }
   ],
   "source": [
    "body_model = LogisticRegression()\n",
    "body_scores = cross_val_score(body_model, body_X, Y, cv=5)\n",
    "\n",
    "print('CV scores: {}'.format(body_scores))\n",
    "\n",
    "print('Average CVScore: {:0.3f} +/- {:0.3f}'.format(body_scores.mean(), body_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining steps together in a pipeline\n",
    "\n",
    "---\n",
    "\n",
    "We can combine these steps to evaluate some future dataset. To properly do this we need to make sure we perform the exact same transformations on the data. \n",
    "\n",
    "For example, if `has_brownies_in_text` is column 19, we need to make sure it is also column 19 during future evaluation.\n",
    "\n",
    "**Pipelines combine both pre-processing and model building steps into a single object**. Rather than manually building transformations and then feeding them into the models, pipelines tie both of these steps together.\n",
    "\n",
    "Similar to models and vectorizers in scikit-learn, pipelines are equipped with\n",
    "\n",
    "- `fit()` methods\n",
    "- `predict()` or `predict_proba()` methods (as any model would be)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Build a pipeline for the title feature data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46800424,  0.53199576],\n",
       "       [ 0.28316267,  0.71683733],\n",
       "       [ 0.00514   ,  0.99486   ],\n",
       "       ..., \n",
       "       [ 0.29063018,  0.70936982],\n",
       "       [ 0.60683954,  0.39316046],\n",
       "       [ 0.66318704,  0.33681296]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually split the data to build a training set\n",
    "training_data = data[:6000]\n",
    "title_X_train = training_data['title'].fillna('')\n",
    "Y_train = training_data['label']\n",
    "\n",
    "# Manually construct the test data\n",
    "title_X_test = data[6000:]['title'].fillna('')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vec', title_vectorizer),\n",
    "        ('model', title_model)\n",
    "    ])\n",
    "\n",
    "# Fit the full pipeline\n",
    "# This means we perform the steps laid out above\n",
    "# First we fit the vectorizer,\n",
    "# And then feed the output of that into the fit function of the model\n",
    "\n",
    "pipeline.fit(title_X_train, Y_train)\n",
    "\n",
    "# Here again we apply the full pipeline for predictions\n",
    "# The text is transformed automatically to match the features from the pipeline\n",
    "pipeline.predict_proba(title_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Add a scaler to the pipeline \n",
    "\n",
    "---\n",
    "\n",
    "As an additional step, we could add a **`MaxAbsScaler`** scaling step to the pipeline, which will occur after the vectorization.\n",
    "\n",
    "**`MaxAbsScaler`** by default transforms the features so that the maximum absolute value of any feature is 1. \n",
    "\n",
    "We have already done this manually but this is an example of doing it in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "ma_scaler = MaxAbsScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vec', title_vectorizer),\n",
    "        ('max_abs_scaler', ma_scaler),\n",
    "        ('model', title_model)\n",
    "    ])\n",
    "\n",
    "pipeline.fit(title_X_train, Y_train)\n",
    "\n",
    "pipeline.predict(title_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merging feature sets in pipelines\n",
    "\n",
    "---\n",
    "\n",
    "While scikit-learn pipelines facilitate transformations of raw data, there may be many steps required before this takes place in your pipeline. These complex pipelines are often referred to as **ETL pipelines for (Extract, Transform, Load)**.\n",
    "\n",
    "In an ETL pipeline, the data is pulled or extracted from some source (like a database), transformed or manipulated, and then loaded into whatever system will analyze the data.\n",
    "\n",
    "Many data science teams rely on software tools to manage these ETL pipelines. If a transformation step fails, these tools alert you, or ensure that step can be re-run. If these transformation steps need to happen daily or weekly, these tools can manage that timeline.\n",
    "\n",
    "One of the most popular Python tools for this is Luigi developed by Spotify:\n",
    "\n",
    "    https://github.com/spotify/luigi\n",
    "\n",
    "Or Airflow by AirBnB:\n",
    "\n",
    "    https://github.com/apache/incubator-airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FeatureUnion\n",
    "\n",
    "---\n",
    "\n",
    "We can also get some of this functionality in sklearn.\n",
    "\n",
    "Let's say, for example, we want to merge many different feature sets together automatically. **`FeatureUnion`** is a very useful tool to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6000x2000 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 40981 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want not only our binary title features, but the word counts as well:\n",
    "\n",
    "title_normcount_vectorizer = CountVectorizer(max_features = 1000,\n",
    "                                             ngram_range=(1, 2),\n",
    "                                             stop_words='english',\n",
    "                                             binary=False)\n",
    "\n",
    "feature_makers = [('binarizer', title_vectorizer), ('normalizer', title_normcount_vectorizer)]\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feature_union = FeatureUnion(feature_makers)\n",
    "\n",
    "comb_features = feature_union.fit_transform(title_X_train)\n",
    "\n",
    "comb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `make_pipeline()` with preprocessing and modeling\n",
    "\n",
    "---\n",
    "\n",
    "Scikit-learn pipelines can also be built perhaps more easily using the `make_pipeline()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import the make_pipeline function\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# StandardScaler is the same as normalization we have been doing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), LogisticRegression())    \n",
    "\n",
    "pipe2 = Pipeline(steps=[('standardscaler',StandardScaler()),\n",
    "                        ('logistic_regr',LogisticRegression())\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pipelines are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logistic_regr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "print pipe1\n",
    "print '\\n--------------------------------------------\\n'\n",
    "print pipe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the preprocessing module\n",
    "\n",
    "---\n",
    "\n",
    "The preprocessing module comes loaded with many very useful pre-processing classes.\n",
    "\n",
    "**Data Manipulators**\n",
    "\n",
    "- Binarizer\n",
    "- KernelCenterer\n",
    "- MaxAbsScaler\n",
    "- MinMaxScaler\n",
    "- Normalizer\n",
    "- OneHotEncoder\n",
    "- PolynomialFeatures\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "**Data Imputation**\n",
    "\n",
    "- Imputer\n",
    "\n",
    "**Function Transformer**\n",
    "\n",
    "- FunctionTransformer\n",
    "\n",
    "**Label Manipulators**\n",
    "\n",
    "- LabelBinarizer\n",
    "- LabelEncoder\n",
    "- MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Custom transformer classes\n",
    "\n",
    "---\n",
    "\n",
    "You can build your own transformers using python classes. This is your first look into python classes, but don't be intimidated!\n",
    "\n",
    "Classes allow you to create **objects containing attributes and functions**, much like how pandas Series objects contain functions such as **`unique()`** as well as attributes such as **`values`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Build a custom transformer class\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# we need to import the template classes to create a class that works like an sklearn class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our \"FeatureMultiplier\" will simply multiply \"X\", the input, by some factor set during initialization:\n",
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return X * self.factor\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 0, 6, 0],\n",
       "       [0, 0, 0, 8]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = FeatureMultiplier(2)\n",
    "\n",
    "test = np.diag((1,2,3,4))\n",
    "print test\n",
    "\n",
    "fm.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0],\n",
       "       [ 0, 12,  0,  0],\n",
       "       [ 0,  0, 18,  0],\n",
       "       [ 0,  0,  0, 24]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like any class with attributes, we can access and change it!\n",
    "\n",
    "print fm.factor\n",
    "\n",
    "fm.factor = 6\n",
    "\n",
    "print fm.factor\n",
    "\n",
    "fm.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice custom transformation classes!\n",
    "\n",
    "---\n",
    "\n",
    "Create a custom transformer that:\n",
    "\n",
    "- Is initialized with a column name for a pandas DataFrame (use `'title'`)\n",
    "- Accepts a pandas DataFrame and returns the column (send in `data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1    The Fully Electronic Futuristic Starting Gun T...\n",
       "2    Fruits that Fight the Flu fruits that fight th...\n",
       "3                  10 Foolproof Tips for Better Sleep \n",
       "4    The 50 Coolest Jerseys You Didn t Know Existed...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def transform(self, dataframe, *_):\n",
    "        return dataframe[self.column]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "ce = ColumnExtractor('title')\n",
    "\n",
    "print ce.column\n",
    "\n",
    "ce.transform(data)[0:5]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
