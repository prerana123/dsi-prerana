{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Feature Selection\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Selection\n",
    "Week 5 | Lesson 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- describe what feature selection means and why it is important\n",
    "- evaluate several techniques for feature selection\n",
    "- use L1 and L2 regularization for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- perform regression\n",
    "- perform classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTOR PREP\n",
    "*Before this lesson, instructors will need to:*\n",
    "- Read in / Review any dataset(s) & starter/solution code\n",
    "- Generate a brief slide deck\n",
    "- Prepare any specific materials\n",
    "- Provide students with additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 mins | [Opening](#opening) | Opening |\n",
    "| 25 mins | [Introduction](#introduction) | Introduction: Feature Selection |\n",
    "| 25 mins | [Demo](#demo) | Demo: Feature importance in Logistic Regression |\n",
    "| 10 mins | [Guided-practice](#guided-practice) | Guided Practice: Visualizing feature importance |\n",
    "| 20 mins | [Ind-practice](#ind-practice) | Independent Practice: Topic |\n",
    "| 5 mins | [Conclusion](#conclusion) | Conclusion |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 mins)\n",
    "In this class we will go over feature selection.\n",
    "Feature selection is an important part of the model building process and there are several ways to approach it.\n",
    "\n",
    "**Check:** Can anyone guess why we may want to select features?\n",
    "\n",
    "> Acceptable answers:\n",
    "- to reduce computational complexity of the model\n",
    "- to reduce memory footprint of dataset\n",
    "- to speed up training\n",
    "- to have better insight into what factors are important\n",
    "- to simplify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: Feature Selection (25 mins)\n",
    "\n",
    "In machine learning we usually start from a vector of features that encapsulate different aspects of our data and we use these to train a model that predicts a target variable.\n",
    "Many of the datasets we have encountered so far were in tabular format, and features were well defined already. This is not usually the case.\n",
    "\n",
    "**Check**Can you give a couple of examples of data where features must be extracted?\n",
    "\n",
    "> Acceptable answers:\n",
    "- Images\n",
    "- Sound\n",
    "- Text\n",
    "- Movies\n",
    "- Timeseries\n",
    "- ...\n",
    "\n",
    "For example, in the case of text data, a data-point usually corresponds to a document or a sentence, and we have to use feature extraction techniques to map each document to a vector of numbers. Similarly, to classify images, we must first represent them as a vector of numbers.\n",
    "As we have seen with the `CountVectorizer` for text data, feature extraction can result in a very large number of features, some of which may have more predictive power than others.\n",
    "\n",
    "|Text|the|cat|is|on|table|blue|sky|...|\n",
    "|----|\n",
    "|The cat is on the table|2|1|1|1|1|0|0|...|\n",
    "|The table is blue|1|0|1|0|1|1|0|...|\n",
    "|The sky is blue|1|0|1|0|0|0|1|...|\n",
    "|...|...|...|...|...|...|...|...|...|\n",
    "\n",
    "In the above example the word `the` probably has zero predictive power.\n",
    "\n",
    "In a different scenario, the tabular data may be a result of a database dump where irrelevant columns were not removed.\n",
    "\n",
    "\n",
    "|CustomerID |CompanyName |ContactName | ContactTitle |Address|City | Region | PostalCode | Country |Phone | Fax|\n",
    "|---|\n",
    "|ALFKI| Alfreds Futterkiste| Maria Anders | Sales Representative | Obere Str. 57 | Berlin|| 12209| Germany | 030-0074321| 030-0076545|\n",
    "|ANATR| Ana Trujillo Emparedados y helados | Ana Trujillo | Owner| Avda. de la Constitución 2222 | México D.F. || 05021| Mexico| (5) 555-4729 | (5) 555-3745|\n",
    "|ANTON| Antonio Moreno Taquería| Antonio Moreno | Owner| Mataderos2312 | México D.F. || 05023| Mexico| (5) 555-3932 |\n",
    "|...|...|...|...|...|...|...|...|...|...|...|\n",
    "\n",
    "Feature Selection is a way to reduce the number of features to simplify the model while retaining its predictive power.\n",
    "\n",
    "\n",
    "\n",
    "### Bottom up feature selection\n",
    "\n",
    "One way to select features is to first find the single feature that gives the highest score and then iteratively add the other features one by one, each time checking how much the score improves. The first features will probably improve the score a lot, while score improvement will be smaller and smaller the more features we add. In other words, adding complexity to the model will yield diminishing returns.\n",
    "\n",
    "One can then set a critearia to only retain the first N features or the first features that achieve 90% as good a score as all the features.\n",
    "\n",
    "### Top Down feature selection \n",
    "\n",
    "Another way to select features is to impose a global constraint on the model, that will force feature selection. For example, in the case of text vectorization, we could impose that a feature needs to have a document frequency higher than a certain threshold to be considered relevant.\n",
    "\n",
    "\n",
    "### Random shuffling\n",
    "\n",
    "A way to check if a feature has any predictive power, is the following. First, we calculate the score of the model, then we randomize the values along that column. If the feature has any predictive power, this should amount to a worse score. On the other hand, if the feature has no predictive power at all, this will result in no change in the score and thus we can toss that feature.\n",
    "\n",
    "**Check** Can anyone restate or recall these definitions in their own words?\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Regularization is an example of a top-down technique that works with parametric models (like Logistic Regression or Support Vector Machine). It imposes a global constraint on the values of the parameters that define the model. The regularized model is found solving a new minimization problem where two terms are present: the term defining the model and the term defining the regularization.\n",
    "\n",
    "In general, a regularization term $R(f)$ is introduced to a general loss function:\n",
    "\n",
    "$$\\min_f \\sum_{i=1}^{n} V(f(\\hat x_i), \\hat y_i) + \\lambda R(f)$$\n",
    "\n",
    "for a loss function $V$ that describes the cost of predicting $f(x)$ when the label is $y$, such as the square loss or hinge loss, and for the term $\\lambda$ which controls the importance of the regularization term. $R(f)$ is typically a penalty on the complexity of $f$, such as restrictions for smoothness or bounds on the vector space norm.\n",
    "\n",
    "#### l1 and l2 regularization\n",
    "\n",
    "For the case where our model $f$ is parametric (e.g. like in linear models), we can express the regularization term as a norm on the parameters. Depending on the kind of norm chosen for the parameters regularization can be:\n",
    "\n",
    "l1 or Lasso regularization:\n",
    "$$ R(f) \\propto \\sum{|\\beta_i|}$$\n",
    "\n",
    "or\n",
    "\n",
    "l2 or Ridge regularization:\n",
    "$$ R(f) \\propto \\sum{\\beta_i^2}$$\n",
    "\n",
    "where $\\beta_i$ are the parameters of the model.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Some models offer an intrinsic way of evaluating feature importance. For example, as we will see next week, _Decision Trees_ have a way to assess how much each feature is contributing to the overall prediction.\n",
    "\n",
    "Logistic Regression also offers an intrinsic way of assessing feature importance. In fact, since the Logistic Regression model is linear in the features, the size of each coefficient represents the impact a unit change in that feature has on the overall model. In order to interpret this as feature importance, we need to make sure that features are normalized to have the same scale, otherwise a unit change in two different feature could actually imply a very different change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Feature importance in Logistic Regression (25 mins)\n",
    "\n",
    "As we have said, the coefficients in logistic regression can be interpreted as feature importance if the data is normalized. We will show this on the Iris dataset for convenience.\n",
    "\n",
    "First load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's initialize a logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` class exposes an attribute called `coef_`. Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41498833,  1.46129739, -2.26214118, -1.0290951 ],\n",
       "       [ 0.41663969, -1.60083319,  0.57765763, -1.38553843],\n",
       "       [-1.70752515, -1.53426834,  2.47097168,  2.55538211]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! According to the documentation this is:\n",
    "\n",
    "    coef_ : array, shape (n_classes, n_features)\n",
    "        Coefficient of the features in the decision function.\n",
    "        \n",
    "Let's display it in a nicer way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>0.414988</td>\n",
       "      <td>1.461297</td>\n",
       "      <td>-2.262141</td>\n",
       "      <td>-1.029095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.416640</td>\n",
       "      <td>-1.600833</td>\n",
       "      <td>0.577658</td>\n",
       "      <td>-1.385538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>-1.707525</td>\n",
       "      <td>-1.534268</td>\n",
       "      <td>2.470972</td>\n",
       "      <td>2.555382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa               0.414988          1.461297          -2.262141   \n",
       "versicolor           0.416640         -1.600833           0.577658   \n",
       "virginica           -1.707525         -1.534268           2.470972   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa             -1.029095  \n",
       "versicolor         -1.385538  \n",
       "virginica           2.555382  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Can we conclude that `petal length (cm)` is the most significant feature to identify `setosa` ?\n",
    "\n",
    "> Answer:\n",
    "No! Since we have not normalized the data, the magnitude of coefficients does not necessarily reflect their importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the data and repeat the exercise:\n",
    "\n",
    "> Student should do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>-0.810166</td>\n",
       "      <td>1.393699</td>\n",
       "      <td>-1.687386</td>\n",
       "      <td>-1.518991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.130380</td>\n",
       "      <td>-1.246338</td>\n",
       "      <td>0.789195</td>\n",
       "      <td>-0.889440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0.012990</td>\n",
       "      <td>-0.144535</td>\n",
       "      <td>1.863173</td>\n",
       "      <td>2.698873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa              -0.810166          1.393699          -1.687386   \n",
       "versicolor           0.130380         -1.246338           0.789195   \n",
       "virginica            0.012990         -0.144535           1.863173   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa             -1.518991  \n",
       "versicolor         -0.889440  \n",
       "virginica           2.698873  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_norm =  StandardScaler().fit_transform(X)\n",
    "\n",
    "model.fit(X_norm, y)\n",
    "\n",
    "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that normalization did change the sign and magnitude of the LR coefficients. Also notice that the Logistic Regression class has a `penalty` parameter that allows us to choose between `l1` and `l2` regularization. Notice that some of the solvers only support `l2` regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Try changing the penalty to `l1`, do the coefficients change?\n",
    "\n",
    "**Optional Check** Check score with `cross_val_score` and select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Visualizing feature importance (10 mins)\n",
    "\n",
    "In some cases we may want to visualize feature importance in a plot. One way to do this is to display the matrix of coefficients with a color scale. We can do this using Matplolib as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD7CAYAAAAfH52VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d05be50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first import matplolib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# then create a figure and a plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# display the matrix\n",
    "cax = ax.matshow(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice, but what if we want to make it a bit clearer to the user?\n",
    "\n",
    "#### Exercise: Add colorbar and ticks\n",
    "\n",
    "Find a way to add colorbars and ticks, it should look like this:\n",
    "\n",
    "![coefficients](./images/coefficients.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer\n",
    "```python\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(coeffs)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(coeffs.columns), rotation=45)\n",
    "ax.set_yticklabels(['']+list(coeffs.index))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: Topic (20 mins)\n",
    "\n",
    "Scikit learn offers several other regularization methods. The following exercise is in pairs:\n",
    "\n",
    "Go to the [documentation page](http://scikit-learn.org/stable/modules/feature_selection.html), pick one method and read about it for 10 minutes.\n",
    "\n",
    "Try to answer the following questions:\n",
    "\n",
    "1. is it top-down or bottom-up?\n",
    "- does it work for both regression and classification?\n",
    "- can it be used in a pipeline?\n",
    "\n",
    "In the last 10 minutes expose your findings to the class\n",
    "\n",
    "> Instructor note:\n",
    "split the class in 10 groups and assign one of these to each group:\n",
    "1. feature_selection.GenericUnivariateSelect([...])\n",
    "- feature_selection.SelectPercentile([...])\n",
    "- feature_selection.SelectKBest([score_func, k])\n",
    "- feature_selection.SelectFpr([score_func, alpha])\n",
    "- feature_selection.SelectFdr([score_func, alpha])\n",
    "- feature_selection.SelectFromModel(estimator)\n",
    "- feature_selection.SelectFwe([score_func, alpha])\n",
    "- feature_selection.RFE(estimator[, ...])\n",
    "- feature_selection.RFECV(estimator[, step, ...])\n",
    "- feature_selection.VarianceThreshold([threshold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "Today we have learned about many ways to select features by importance. This will be particularly useful when we deal with a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Scikit Learn Documentation on Feature Selection](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [Matplotlib documentation](http://matplotlib.org/api/pyplot_api.html)\n",
    "- [Wikipedia page on regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
