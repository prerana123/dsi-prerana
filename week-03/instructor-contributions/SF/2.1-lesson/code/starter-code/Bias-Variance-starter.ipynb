{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Tradeoff\n",
    "\n",
    "In this notebook we investigate the bias and variance of models. In the first section we'll investigate fitting functions of the form:\n",
    "\n",
    "$$f(x) = a + b * x^n$$\n",
    "\n",
    "to some cubic data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "The following functions will generate data and polynomial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polynomials(X, degree=1):\n",
    "    array = [X]\n",
    "    y = X\n",
    "    for i in range(2, int(degree + 1)):\n",
    "        y = y * X\n",
    "        array.append(y)\n",
    "    return np.array(array).transpose()\n",
    "\n",
    "def monomials(X, degree=1):\n",
    "    y = np.array(X)\n",
    "    y = np.power(X, degree)\n",
    "    return np.array([y]).transpose()\n",
    "\n",
    "def generate_data(func, a, b, n=100):\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        x = random.random() * (b - a)\n",
    "        y = func(x)\n",
    "        data.append((x, y))\n",
    "    return list(sorted(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0],\n",
       "       [  1,   1,   1],\n",
       "       [  2,   4,   8],\n",
       "       [  3,   9,  27],\n",
       "       [  4,  16,  64],\n",
       "       [  5,  25, 125],\n",
       "       [  6,  36, 216],\n",
       "       [  7,  49, 343],\n",
       "       [  8,  64, 512],\n",
       "       [  9,  81, 729]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(range(10))\n",
    "polynomials(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "# norm = np.random.normal(mu, sigma)\n",
    "\n",
    "# Generate some data\n",
    "f = lambda x: 4 + 5 * x - 3 * x*x + 0.1 * x * x * x + np.random.normal(mu, sigma)\n",
    "data = generate_data(f, 0, 2, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Polynomial Functions\n",
    "\n",
    "Here we fit try to fit our sample data with an increasing exponent:\n",
    "\n",
    "$$f(x) = 1 + b x^n$$\n",
    "\n",
    "None of these models are quite right because the data is generate from the function\n",
    "\n",
    "$$ \\hat{f}(x) = 4 + 5 x - 3 x^2 + 0.1 x^3$$\n",
    "\n",
    "So each model is not quite right because one or more terms is missing. This will allow us to take a closer look at the bias-variance tradeoff.\n",
    "\n",
    "Let's start with $n = 1, 2, 3, 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = np.array([x[0] for x in data])\n",
    "Y = np.array([x[1] for x in data])\n",
    "\n",
    "for i in range(1, 10):\n",
    "    X = monomials(domain, i)\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, Y)\n",
    "\n",
    "    yhat = regr.predict(X)\n",
    "    \n",
    "    bias = np.mean((yhat - Y)**2)\n",
    "    var = np.var(yhat)\n",
    "    \n",
    "    # The coefficients\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance explained score: %.2f' % regr.score(X, Y))\n",
    "\n",
    "    # The mean square error\n",
    "    print(\"Residual sum of squares: %.2f\" % sse)\n",
    "\n",
    "    print(\"Bias: {bias}\".format(bias=bias))\n",
    "    print(\"Variance: {var}\".format(var=var))\n",
    "        \n",
    "    # Plot outputs\n",
    "    plt.scatter(domain, Y,  color='black')\n",
    "    plt.plot(domain, regr.predict(X), color='blue', linewidth=3)\n",
    "\n",
    "    plt.title(\"n = \" + str(i))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the true measures of bias and variance are averaged over many fits to many samples from the same data source, and we're only looking at one instance here. Notice that the bias is at first decreasing as the exponent increases, but at $n=6$ we see the bias start to increase while the squared errors are basically unchanged. Let's make a plot of the bias as the exponent increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = []\n",
    "vars = []\n",
    "exps = np.arange(0, 10, 0.1)\n",
    "\n",
    "for i in exps:\n",
    "    X = monomials(domain, i)\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, Y)\n",
    "\n",
    "    yhat = regr.predict(X)\n",
    "    \n",
    "    bias = np.mean((yhat - Y)**2)\n",
    "    var = np.var(yhat)\n",
    "    \n",
    "    bs.append(bias)\n",
    "    vars.append(var)\n",
    "\n",
    "plt.plot(exps, bs, label=\"Bias\")\n",
    "plt.plot(exps, vars, label=\"Variance\")\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just as in the conceptual diagram from the lesson, there's a spot where the bias is minimized around $n=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance\n",
    "\n",
    "Now let's try to fit the full polynomial as the degree increases, which will help us visualize variance error.\n",
    "\n",
    "**Check**: In the next code section, what changes when compared to the code above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = np.array([x[0] for x in data])\n",
    "Y = np.array([x[1] for x in data])\n",
    "\n",
    "for i in range(1, 10):\n",
    "    X = polynomials(domain, i)\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, Y)\n",
    "\n",
    "    yhat = regr.predict(X)\n",
    "    \n",
    "    bias = np.mean((yhat - Y)**2)\n",
    "    var = np.var(yhat)\n",
    "    \n",
    "    # The coefficients\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % regr.score(X, Y))\n",
    "\n",
    "    # The mean square error\n",
    "    print(\"Residual sum of squares: %.2f\" % sse)\n",
    "\n",
    "    print(\"Bias: {bias}\".format(bias=bias))\n",
    "    print(\"Variance: {var}\".format(var=var))\n",
    "        \n",
    "    # Plot outputs\n",
    "    plt.scatter(domain, Y,  color='black')\n",
    "    plt.plot(domain, regr.predict(X), color='blue', linewidth=3)\n",
    "\n",
    "    plt.title(\"n = \" + str(i))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = []\n",
    "vars = []\n",
    "exps = np.arange(0, 12, .1)\n",
    "\n",
    "for i in exps:\n",
    "    X = polynomials(domain, i)\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, Y)\n",
    "\n",
    "    yhat = regr.predict(X)\n",
    "    \n",
    "    bias = np.mean((yhat - Y)**2)\n",
    "    var = np.var(yhat)\n",
    "    \n",
    "    bs.append(bias)\n",
    "    vars.append(var)\n",
    "\n",
    "plt.plot(exps, bs, label=\"Bias\")\n",
    "plt.plot(exps, vars, label=\"Variance\")\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the fits for $n = 2, 3, 4$ are pretty good. As the exponent increases, the best fit curve is overfitting the dataset, and the shape of the curve does not reflect what we know the underlying data to really look like. In this case we have error due to variance -- with too many parameters, our model is fitting the random variations in the data which we generally want to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Practice\n",
    "\n",
    "We've seen that as the complexity in our models change so too do the bias and variance. Let's investigate from another angle using linear regression. Rather than change the model, we'll change the underlying data to be drawn from a higher dimensional model.\n",
    "\n",
    "Your tasks are:\n",
    "* Fill in the code to fit a linear regression to the data\n",
    "* Investiage the bias and variance as the data source changes in complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for degree in range(1, 7):\n",
    "    # Generate some data\n",
    "    f = lambda x: 1 - 2 * x + 3 * x ** degree + np.random.normal(mu, sigma)\n",
    "    data = generate_data(f, 0, 2, n=20)\n",
    "\n",
    "    domain = np.array([x[0] for x in data])\n",
    "    Y = np.array([x[1] for x in data])\n",
    "\n",
    "    X = np.array([domain]).transpose()\n",
    "    # Create linear regression object\n",
    "\n",
    "    # Compute the errors\n",
    "\n",
    "    # Plot outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Exercises\n",
    "\n",
    "Some functions, like $sin$ and $exp$ can be better fit with increasing large polynomials.\n",
    "\n",
    "* By modifying the above code, fit polynomials of degree $n$ as $n$ ranges from 1 to 10 to $sin$ on the interval $[0, 2 \\pi]$.\n",
    "* Can you explain why the bias and variance continue to decrease as $n$ gets large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "f = lambda x: math.sin(x) + np.random.normal(mu, sigma)\n",
    "data = generate_data(f, 0, 2*3.14, n=100)\n",
    "\n",
    "domain = np.array([x[0] for x in data])\n",
    "Y = np.array([x[1] for x in data])\n",
    "\n",
    "for i in range(1, 10):\n",
    "    pass\n",
    "    # Fit a polynomial of degree i to the data\n",
    "    \n",
    "    # Compute the errors\n",
    "\n",
    "    # Plot outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the same exercise with the function\n",
    "\n",
    "$$ y = e^x $$\n",
    "\n",
    "You should again find that larger polynomials fit the data better.\n",
    "\n",
    "### A bad model for the exponential\n",
    "\n",
    "To see a model with high variance, try fitting the model:\n",
    "\n",
    "```\n",
    "y = a sin(x) + b cos(x)\n",
    "```\n",
    "\n",
    "to `y = e^x`. Does this model have more bias or variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
