{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px;\">\n",
    "# Evaluating Model Fit\n",
    "---\n",
    "Week 4 Lesson 2.3\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Explain the fundamentals of evaluating classifiers\n",
    "- Define precision, recall, accuracy, f1-score, and ROC curves\n",
    "- Use sklearn.metrics functions to compute these metrics for a fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min  | [Opening](#opening)  | How to use scikit-learn to run logistic  |\n",
    "| 5 min  | [Introduction](#introduction)   | Topic description  |\n",
    "| 30 min  | [Guided Practice](#guided-practice<a name=\"opening\"></a>)  | Exploring classifier evaluation metrics  |\n",
    "| 15 min  | [Demo](#demo)  | AUC and ROC curves |\n",
    "| 25 min  | [Independent Practice](#ind-practice)  | Practice AUC and ROC curves |\n",
    "| 5 min  | [Conclusion](#conclusion)  | Topic description  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening: Run logistic regression with sklearn (5 mins)\n",
    "\n",
    "Like statsmodels, sklearn also has a function to run logistic regression. The implementation is different than statsmodels but is easy use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Set up test data for X and Y\n",
    "X = np.random.randn(100, 3)\n",
    "Y = np.random.binomial(1, 0.5, 100)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# coefficients:\n",
    "logreg.coef_\n",
    "\n",
    "X_new = np.random.randn(100, 3)\n",
    "\n",
    "# predict class:\n",
    "y_pred = logreg.predict(X_new)\n",
    "\n",
    "# predicted probability:\n",
    "y_pp = logreg.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The sklearn implementation of logistic regression is focused more on flexibility\n",
    "than the statsmodels implementation, but does not return as many summary\n",
    "statistics. We will be using the sklearn implementation for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://marginalrevolution.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: Topic (5 mins)\n",
    "\n",
    "Classification problems and models are evaluated differently than regression models. Whereas regression models predict a continuous variable, classification problems predict probability of belonging to a class of outcome.\n",
    "\n",
    "Instead of evaluating models based on error like in regression, we evaluate the models based on the correct and incorrect labeling of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most classification metrics are based on four outcome categories during prediction:\n",
    "\n",
    "- **True Positives:** A positive class observation (1) is correctly classified as positive by the model.\n",
    "- **False Positive:** A negative class observation (0) is incorrectly classified as positive.\n",
    "- **True Negative:** A negative class observation is correctly classified as negative.\n",
    "- **False Negative:** A positive class observation is incorrectly classified as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring classifier evaluation metrics (25 min)\n",
    ".\n",
    "For this section, let's say we have predicted Y for a test set from a logistic regression trained on a train set.\n",
    "\n",
    "You can follow along in the ipython notebook file:\n",
    "\n",
    "[Evaluation metrics guided practice](./code/guided-practice.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification evaluation metric fundamentals\n",
    "\n",
    "##### Confusion matrix\n",
    "\n",
    "  The confusion matrix is very basic, yet contains all of the information required for calculating more complex evaluation metrics.\n",
    "\n",
    "  A confusion matrix has as rows and columns the classes modeled by your classifier.  In the case of logistic regression this will be a 2x2 matrix.  Rows indicate the actual class, and columns indicate the predicted class.\n",
    "  \n",
    "|                | Predicted Cancer | Predicted Healthy |\n",
    "| --------------:|:----------------:|:-----------------:|\n",
    "| **Has Cancer** | 168              | 31                |\n",
    "| **Is Healthy** | 46               | 85                |\n",
    "\n",
    "\n",
    "From the 2-variable confusion matrix we can calculate **true positives**, **false positives**, **true negatives**, and **false negatives** directly from the cells. **Tuning your model to adjust these metrics depends on your priorities.** In healthcare for example, you may want to minimize false positives at the expense of false negatives.\n",
    "\n",
    "> Check: how many patients in the dataset have cancer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which metrics describe:\n",
    "\n",
    "- true positives\n",
    "- false positives\n",
    "- true negatives\n",
    "- false negatives\n",
    "\n",
    "|                | Predicted Cancer | Predicted Healthy |\n",
    "| --------------:|:----------------:|:-----------------:|\n",
    "| **Has Cancer** | 168              | 31                |\n",
    "| **Is Healthy** | 46               | 85                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/images/c_matrix_metrics.001.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = np.array(confusion_matrix(Y_test, Y_pred))\n",
    "\n",
    "print(confusion)\n",
    "\n",
    "# calculate true positives, the number of 1s correctly predicted to be 1\n",
    "TP = confusion[0,0]\n",
    "\n",
    "# calculate false positives, the number of 0s incorrectly predicted to be 1\n",
    "FP  = confusion[1,0]\n",
    "\n",
    "# calculate true negatives, the number of 0s correctly predicted to be 0\n",
    "TN = confusion[1,1]\n",
    "\n",
    "# calculate false negatives, the number of 1s incorrectly predicted to be 0\n",
    "FN = confusion[0,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Accuracy\n",
    "\n",
    "Accuracy is simply the proportion of classes correctly predicted by the model.\n",
    "\n",
    " \n",
    ">Accuracy = (True Positives + True Negatives) / Total\n",
    " \n",
    "```python\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# This is equivalent to:\n",
    "acc = np.sum(Y_test == Y_pred)/len(Y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### sklearn classification report\n",
    "\n",
    "  The classification report function returns a detailed printout of metrics about your model.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.metrics import classification_report\n",
    "\n",
    "  # Example printout:\n",
    "  print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "                   precision recall    f1-score   support\n",
    "\n",
    "  0                 0.79      0.84      0.81       199\n",
    "  1                 0.73      0.65      0.69       131\n",
    "\n",
    "  avg / total       0.76      0.77      0.76       330\n",
    "  ```\n",
    "_The 0 and 1 on the left column indicate the two classes predicted by the model. For models with multiple classes there would be more rows and labels._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each of the columns indicate an important metric for evaluating classification model performance.\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "|**precision** | Ability of the classifier to avoid labeling a class as a member of another class. <br><br> `Precision = True Positives / (True Positives + False Positives)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class.  precision score of 0 would mean that the classifier misclassified every instance of the current class_ |\n",
    "|**recall**    | is the ability of the classifier to correctly identify the current class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**f1-score** | is the harmonic mean of the precision and recall. The harmonic mean is used here rather than the more conventional arithmetic mean because the harmonic mean is more appropriate for averaging rates. <br><br>`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)` <br><br>_The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once._ |\n",
    "|**support** | is simply the number of observations of the labelled class.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo / Codealong: AUC and ROC curves (15 mins)\n",
    "\n",
    "Open the jupyter notebook for the codealong to learn about area under the curve (AUC) and the receiver operating characteristic curve (ROC).\n",
    "\n",
    "\n",
    "[AUC ROC codealong](./code/AUC-ROC-codealong.ipynb)\n",
    "\n",
    "\n",
    "**Check for understanding:**\n",
    "- What are some reasons that you might change the class assignment threshold in a classifier?\n",
    "- How does changing the threshold for assigning a class label affect the confusion matrix?\n",
    "- How is the ROC related to the confusion matrix?\n",
    "- Why is the ROC curve unaffected by the distribution of classes in the data?\n",
    "- Is the highest AUC model always the best model? What if you weight the costs of FP very differently from FN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: metrics and ROC with housing data (25 minutes)\n",
    "\n",
    "Practice classification evaluation metrics and plotting ROC curves with the Sacramento housing data. Load the starter code below to begin.\n",
    "\n",
    "[AUC housing starter code](./code/AUC-sacramento-housing-starter-code.ipynb)\n",
    "\n",
    "> Check: can you measure and explain your results to a partner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "- There are many metrics for predictive model performance\n",
    "- ROC charts and AUC scores are common tools for summarizing performance dynamics for binary classifiers\n",
    "- Ultimately, YOU need to decide what to measure and what to prioritize; this should depend on context\n",
    "\n",
    "Suggested further reading: http://blog.mldb.ai/blog/posts/2016/01/ml-meets-economics/\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
