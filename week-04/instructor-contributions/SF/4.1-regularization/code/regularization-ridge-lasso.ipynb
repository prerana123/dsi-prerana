{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization with Ridge and Lasso\n",
    "\n",
    "This notebook visualizes the effect of ridge and lasso on the wine dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Import data\n",
    "\n",
    "Load in the wine dataset with pandas. This version has already had the red and white wines concatenated together and tagged with a binary 1,0 indicator (1 is red wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv('../assets/datasets/winequality_merged.csv')\n",
    "\n",
    "# replace spaces in column names and convert all columns to lowercase:\n",
    "wine.columns = [x.lower().replace(' ','_') for x in wine.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Normalize the predictor columns\n",
    "\n",
    "With the Lasso and Ridge it is very neccessary to normalize the predictor columns before constructing the models, even the dummy coded categorical variables. Below we define our target variable and then normalize the columns that are not the target.\n",
    "\n",
    "**Q. Can you explain why normalization is so important in the context of regularization?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a target/dependent variable that we will predict\n",
    "target = 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select all the columns that are not the target\n",
    "nc = [x for x in wine.columns if x != target]\n",
    "\n",
    "# Using the .ix indexing syntax, subtract mean and divide by standard deviation for all predictor columns.\n",
    "#\n",
    "# Remember: .ix indexing notation works like: data.ix[row_indices, column_indices]\n",
    "# .ix is able to take a mix of boolean, number, or string specifications, which is useful.\n",
    "#\n",
    "# By subtracting the mean and dividing by the standard devation, the normalization procedure is putting \n",
    "# all of the predictor variables on the same scale (distributions with mean == 0 and standard deviation == 1)\n",
    "wine.ix[:, nc] = (wine.ix[:, nc] - wine.ix[:, nc].mean()) / wine.ix[:, nc].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Check out the correlation matrix\n",
    "\n",
    "Correlation matrix is useful to get some intuition right away for the relationships (or lack thereof) between all of our variables.\n",
    "\n",
    "Print out the numeric correlation. Also make a heatmap with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine_corr = wine.corr()\n",
    "wine_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the default matplotlib figure size to 7x7:\n",
    "plt.rcParams['figure.figsize']=(9,7)\n",
    "\n",
    "# Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
    "mask = np.zeros_like(wine_corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Plot the heatmap with seaborn.\n",
    "# Assign the matplotlib axis the function returns. This will let us resize the labels.\n",
    "ax = sns.heatmap(wine_corr, mask=mask)\n",
    "\n",
    "# Resize the labels.\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14)\n",
    "\n",
    "# If you put plt.show() at the bottom, it prevents those useless printouts from matplotlib.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Make X and Y data\n",
    "\n",
    "Import patsy so we can use the formula syntax.\n",
    "\n",
    "We will be making two X, Y datasets. \n",
    "\n",
    "1. The first is going to just be all the variables added together.\n",
    "2. The second will 'multiply' the variables together. This means that every possible interaction between variables will be modeled, which should overfit the target quite a bit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "# Get the non-target cols with a simple list comprehension\n",
    "non_target_cols = [c for c in wine.columns if c != target]\n",
    "\n",
    "# Use some string adding and joining to make the simple model formula:\n",
    "formula_simple = target + ' ~ ' + ' + '.join(non_target_cols) + ' -1'\n",
    "print formula_simple\n",
    "\n",
    "# Make the complex formula:\n",
    "formula_complex = target + ' ~ (' + ' + '.join(non_target_cols) + ')**2 -1'\n",
    "print formula_complex\n",
    "\n",
    "# Create the X and Y pairs for both!\n",
    "Y, X = patsy.dmatrices(formula_simple, data=wine)\n",
    "Yoverfit, Xoverfit = patsy.dmatrices(formula_complex, data=wine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how the shapes of the X matrices differ. You can see taht the Xoverfit has a ton of columns due to the creation of all possible interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape\n",
    "print Xoverfit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the feature names for the simple and complex models to variables. We can use the `.design_info.column_names` attribute of these patsy matrix objects.\n",
    "\n",
    "You can print out the complex feature names if you feel like it. There are a ton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_feature_names = X.design_info.column_names\n",
    "complex_feature_names = Xoverfit.design_info.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Visualize the Ridge and Lasso regularization on coefficients\n",
    "\n",
    "We will only do this for the simple data since the model that will overfit has too many coefficients to see anything properly on a graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Visualizing the Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `Ridge` model class (a \"blueprint\" for the model) from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I write a function that iterates over a series of different alpha regularization parameters. The alpha is related to the lambda value that multiples the square of betas from the equation (though it may not be exactly the same numerically â€“ regardless, increasing the alpha increases the regularization of the ridge on the coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_coefs(X, Y, alphas):\n",
    "    \n",
    "    # set up the list to hold the different sets of coefficients:\n",
    "    coefs = []\n",
    "    \n",
    "    # Set up a ridge regression object\n",
    "    ridge_reg = Ridge()\n",
    "    \n",
    "    # Iterate through the alphas fed into the function:\n",
    "    for a in alphas:\n",
    "        \n",
    "        # On each alpha reset the ridge model's alpha to the current one:\n",
    "        ridge_reg.set_params(alpha=a)\n",
    "        \n",
    "        # fit or refit the model on the provided X, Y\n",
    "        ridge_reg.fit(X, Y)\n",
    "        \n",
    "        # Get out the coefficient list (first element of the .coef_ attribute)\n",
    "        coefs.append(ridge_reg.coef_[0])\n",
    "        \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha values for the ridge are best visualized on a logarithmic \"magnitude\" scale. Essentially the effect does not increase linearly so much as by the increase in orders of magnitude. You will see that this is different for the lasso (and is related to the fact that for the Ridge the beta coefficients are squared, rather than just the absolute value like in the Lasso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.logspace gives us points between specified orders of magnitude on a logarithmic scale. It is base 10.\n",
    "r_alphas = np.logspace(0, 5, 200)\n",
    "\n",
    "# Get the coefficients for each alpha for the Ridge, using the function above\n",
    "r_coefs = ridge_coefs(X, Y, r_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Below I've written a plotting function that will:\n",
    "\n",
    " - Plot the effect of changing alpha on the coefficient size on a **path** graph\n",
    " - Plot the effect of changing alpha on the coefficient size on a **bar** graph\n",
    " \n",
    "Each one gives informative information. It's just two different ways of visualizing the same thing. The chart is interactive so you can play around with the values of alpha across the specified range above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The cycler package lets us \"cycle\" throug colors.\n",
    "# Just another thing i had to look up on stackoverflow. That's my life.\n",
    "from cycler import cycler\n",
    "\n",
    "def coef_plotter(alphas, coefs, feature_names, to_alpha, regtype='ridge'):\n",
    "    \n",
    "    # Get the full range of alphas before subsetting to keep the plots from \n",
    "    # resetting axes each time. (We use these values to set static axes later).\n",
    "    amin = np.min(alphas)\n",
    "    amax = np.max(alphas)\n",
    "    \n",
    "    # Subset the alphas and coefficients to just the ones below the set limit\n",
    "    # from the interactive widget:\n",
    "    alphas = [a for a in alphas if a <= to_alpha]\n",
    "    coefs = coefs[0:len(alphas)]\n",
    "    \n",
    "    # Get some colors from seaborn:\n",
    "    colors = sns.color_palette(\"husl\", len(coefs[0]))\n",
    "    \n",
    "    # Get the figure and reset the size to be wider:\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,7)\n",
    "\n",
    "    # We have two axes this time on our figure. \n",
    "    # The fig.add_subplot adds axes to our figure. The number inside stands for:\n",
    "    #[figure_rows|figure_cols|position_of_current_axes]\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Give it the color cycler:\n",
    "    ax1.set_prop_cycle(cycler('color', colors))\n",
    "    \n",
    "    # Print a vertical line showing our current alpha threshold:\n",
    "    ax1.axvline(to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    \n",
    "    # Plot the lines of the alphas on x-axis and coefficients on y-axis\n",
    "    ax1.plot(alphas, coefs, lw=2)\n",
    "    \n",
    "    # set labels for axes:\n",
    "    ax1.set_xlabel('alpha', fontsize=20)\n",
    "    ax1.set_ylabel('coefficients', fontsize=20)\n",
    "    \n",
    "    # If this is for the ridge, set this to a log scale on the x-axis:\n",
    "    if regtype == 'ridge':\n",
    "        ax1.set_xscale('log')\n",
    "    \n",
    "    # Enforce the axis limits:\n",
    "    ax1.set_xlim([amin, amax])\n",
    "    \n",
    "    # Put a title on the axis\n",
    "    ax1.set_title(regtype+' coef paths\\n', fontsize=20)\n",
    "    \n",
    "    # Get the ymin and ymax for this axis to enforce it to be the same on the \n",
    "    # second chart:\n",
    "    ymin, ymax = ax1.get_ylim()\n",
    "\n",
    "    # Add our second axes for the barplot in position 2:\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # Position the bars according to their index from the feature names variable:\n",
    "    ax2.bar(range(1, len(feature_names)+1), coefs[-1], align='center', color=colors)\n",
    "    ax2.set_xticks(range(1, len(feature_names)+1))\n",
    "    \n",
    "    # Reset the ticks from numbers to acutally be the names:\n",
    "    ax2.set_xticklabels(feature_names, rotation=65, fontsize=12)\n",
    "    \n",
    "    # enforce limits and add titles, labels\n",
    "    ax2.set_ylim([ymin, ymax])\n",
    "    ax2.set_title(regtype+' predictor coefs\\n', fontsize=20)\n",
    "    ax2.set_xlabel('coefficients', fontsize=20)\n",
    "    ax2.set_ylabel('alpha', fontsize=20)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ipython widgets so we can make this plotting function interactive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and `interact` from ipywidgets lets me take some specified alphas that we have already calculated the coefficients for and plot them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_plot_runner(log_of_alpha=0):\n",
    "    coef_plotter(r_alphas, r_coefs, simple_feature_names, 10**log_of_alpha, regtype='ridge')\n",
    "\n",
    "interact(ridge_plot_runner, log_of_alpha=(0.0,5.0,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Visualizing the Lasso\n",
    "\n",
    "Now we do the same thing as above but for the Lasso. You will be able to see how the coefficients change differently for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the same as the ridge coefficient by alpha calculator\n",
    "def lasso_coefs(X, Y, alphas):\n",
    "    coefs = []\n",
    "    lasso_reg = Lasso()\n",
    "    for a in alphas:\n",
    "        lasso_reg.set_params(alpha=a)\n",
    "        lasso_reg.fit(X, Y)\n",
    "        coefs.append(lasso_reg.coef_)\n",
    "        \n",
    "    return coefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alphas for the Lasso don't have their effects by orders of magnitude. A linear series of alphas is sufficient. Don't worry about the warning for Lasso at alpha 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_alphas = np.arange(0, 0.2, 0.0025)\n",
    "l_coefs = lasso_coefs(X, Y, l_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same plotting function above, but now with the calculated coefficients by alpha for the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso_plot_runner(alpha=0):\n",
    "    coef_plotter(l_alphas, l_coefs, simple_feature_names, alpha, regtype='lasso')\n",
    "\n",
    "interact(lasso_plot_runner, alpha=(0.0,0.2,0.0025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Model performance of the Ridge and Lasso on the simple data\n",
    "\n",
    "Let's check out how the penalties affect the performance of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the KFold crossvalidation function from sklearn. We'll make five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kfolds = KFold(wine.shape[0], n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the Ridge and Lasso, iterate through their alphas and using the cross-validation folds calculate the average R2 at each regularization level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_lasso_cv_rsq(X, Y, r_alphas, l_alphas, kfolds, verbose=False):\n",
    "    \n",
    "    # lists to track mean R2s per alpha\n",
    "    ridge_rsq = []\n",
    "    lasso_rsq = []\n",
    "\n",
    "    # initialize models\n",
    "    lasso = Lasso()\n",
    "    ridge = Ridge()\n",
    "\n",
    "    print 'Lasso...'\n",
    "    # iterate through Lasso alphas\n",
    "    for la in l_alphas:\n",
    "        \n",
    "        if verbose: print la\n",
    "        \n",
    "        # set the current alpha to the model\n",
    "        lasso.set_params(alpha=la)\n",
    "        \n",
    "        # keep track of fold R2s\n",
    "        rsqs = []\n",
    "        \n",
    "        # iterate through the folds. Each iteration returns the training and\n",
    "        # testing indices\n",
    "        for traini, testi in kfolds:\n",
    "            \n",
    "            # run the current model with the subset training X and Y\n",
    "            lasso.fit(X[traini], Y[traini])\n",
    "            \n",
    "            # append the R2 on the test set to the tracker\n",
    "            rsqs.append(lasso.score(X[testi], Y[testi]))\n",
    "            \n",
    "        # append the mean of the R2s for this alpha to the R2 by alpha list\n",
    "        lasso_rsq.append(np.mean(rsqs))\n",
    "\n",
    "    print 'Ridge...'\n",
    "    # Do the same process as above for ridge...\n",
    "    for ra in r_alphas:\n",
    "        \n",
    "        if verbose: print ra\n",
    "        \n",
    "        ridge.set_params(alpha=ra)\n",
    "        rsqs = []\n",
    "        for traini, testi in kfolds:\n",
    "            ridge.fit(X[traini], Y[traini])\n",
    "            rsqs.append(ridge.score(X[testi], Y[testi]))\n",
    "        ridge_rsq.append(np.mean(rsqs))\n",
    "        \n",
    "    return ridge_rsq, lasso_rsq\n",
    "    \n",
    "# Get the ridge and lasso cross-validated R2s:\n",
    "ridge_rsq, lasso_rsq = ridge_lasso_cv_rsq(X, Y, r_alphas, l_alphas, kfolds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Below we have a new plotting function that will track the performance of the model as the alphas increase, as measured by the mean R2s across cross-validation folds. Remember that R2 is a measure of how much variance in the target/dependent variable is explained by our predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rsq_plotter(ridge_alphas, ridge_to_alpha, ridge_rsq,\n",
    "                lasso_alphas, lasso_to_alpha, lasso_rsq):\n",
    "    \n",
    "    \n",
    "    # Find the overall minimum and maximum alpha values for\n",
    "    # the Ridge and Lasso to fix the plot axes:\n",
    "    ridge_amin = np.min(ridge_alphas)\n",
    "    ridge_amax = np.max(ridge_alphas)\n",
    "    \n",
    "    lasso_amin = np.min(lasso_alphas)\n",
    "    lasso_amax = np.max(lasso_alphas)\n",
    "    \n",
    "    # Subet the models' alphas and rsqs according to the currently set\n",
    "    # alpha limits for each (passed in from the interactive sliders)\n",
    "    ridge_alphas = [a for a in ridge_alphas if a <= ridge_to_alpha]\n",
    "    ridge_rsq = ridge_rsq[0:len(ridge_alphas)]\n",
    "    \n",
    "    lasso_alphas = [a for a in lasso_alphas if a <= lasso_to_alpha]\n",
    "    lasso_rsq = lasso_rsq[0:len(lasso_alphas)]\n",
    "    \n",
    "    # Get some unique colors out for the Ridge R2 line, Lasso R2 line,\n",
    "    # and the 'max R2 achieved' line.\n",
    "    colors = sns.xkcd_palette(['windows blue', 'amber', 'faded green'])\n",
    "\n",
    "    # We will again be plotting two axes on the same figure:\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,7)\n",
    "\n",
    "    # The first subplot axes is for the ridge\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Plot a vertical line indicating the current alpha selected\n",
    "    ax1.axvline(ridge_to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    \n",
    "    # Plot a horizontal line for the maximum R2 achieved so far across alphas\n",
    "    # for the Ridge\n",
    "    ax1.axhline(np.max(ridge_rsq), lw=2, c=colors[2], alpha=0.8)\n",
    "    \n",
    "    # Plot the line tracking R2s by alpha values\n",
    "    ax1.plot(ridge_alphas, ridge_rsq, lw=3, c=colors[0])\n",
    "    \n",
    "    # Add the axis labels\n",
    "    ax1.set_xlabel('ridge alpha', fontsize=20)\n",
    "    ax1.set_ylabel('ridge CV R2', fontsize=20)\n",
    "    \n",
    "    # Set x-axis to logarithmic scale\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Fix the axes in place\n",
    "    ax1.set_xlim([ridge_amin, ridge_amax])\n",
    "    ax1.set_ylim([-0.05, 1])\n",
    "    \n",
    "    # set the title for the axes;\n",
    "    ax1.set_title('ridge cross-val performance\\n', fontsize=20)\n",
    "    \n",
    "    \n",
    "    # Now do all of this as above for the Lasso!\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.axvline(lasso_to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    ax2.axhline(np.max(lasso_rsq), lw=2, c=colors[2], alpha=0.8)\n",
    "    \n",
    "    ax2.plot(lasso_alphas, lasso_rsq, lw=3, c=colors[0])\n",
    "    \n",
    "    ax2.set_xlabel('lasso alpha', fontsize=20)\n",
    "    ax2.set_ylabel('lasso CV R2', fontsize=20)\n",
    "    ax2.set_xlim([lasso_amin, lasso_amax])\n",
    "    ax2.set_ylim([-0.05, 1])\n",
    "        \n",
    "    ax2.set_title('lasso cross-val performance\\n', fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have our new widget where you can change both the Ridge and Lasso alphas to see how they compare:\n",
    "\n",
    "def rsq_plot_pipe(ra, la):\n",
    "    rsq_plotter(r_alphas, 10**ra, ridge_rsq, l_alphas, la, lasso_rsq)\n",
    "    \n",
    "w = widgets.interactive(rsq_plot_pipe, \n",
    "                        ra=widgets.FloatSlider(value=0, min=0., max=5., step=0.05, description='Ridge log10(alpha):'),\n",
    "                        la=widgets.FloatSlider(value=0, min=0., max=0.2, step=0.0025, description='Lasso alpha:')\n",
    ")\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. Model performance on the complex data\n",
    "\n",
    "Let's see what happens when we do this for the complex data that is designed to overfit.\n",
    "\n",
    "[This function takes a long time to run by comparison, as you might expect.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overfit_ridge_rsq, overfit_lasso_rsq = ridge_lasso_cv_rsq(Xoverfit, Yoverfit, \n",
    "                                                          r_alphas, l_alphas, \n",
    "                                                          kfolds, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
