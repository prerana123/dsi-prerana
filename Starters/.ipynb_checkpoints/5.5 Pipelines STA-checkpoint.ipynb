{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "Data pipelines are a series of automated data transformations that ensure the validity of your work for routine data maintenance tasks. Each stage of a pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end just as water flows through a pipeline. Many organizations rely on data engineering teams to encode common tasks into pipelines.\n",
    "\n",
    "Examples of data transformations:\n",
    "- change in scale, units, or base\n",
    "- text vectorization\n",
    "- image vectorization\n",
    "- sound file vectorization\n",
    "- missing data imputation\n",
    "- clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"data/stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "# fill NA with empty cells and check data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set label as target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check target proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# countvectorize our first title\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how Count Vectorizer works:\n",
    "![Example](assets/CountVectorizer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vectorize our original training title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary of the titles\n",
    "\n",
    "# Use `transform` to generate the sample X word matrix - one column per feature (word or n-grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build Logit and CV score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set\n",
    "\n",
    "\n",
    "# reserve future data, unavailable at training time\n",
    "\n",
    "# Fit the full pipeline\n",
    "\n",
    "# This means we perform the steps laid out above\n",
    "# First we fit the vectorizer,\n",
    "# And then feed the output of that into the fit function of the model\n",
    "\n",
    "\n",
    "# Here again we apply the full pipeline for predictions\n",
    "# The text is transformed automatically to match the features from the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Feature Sets in Pipelines\n",
    "\n",
    "We may want to merge many different feature sets automatically. Here we can use scikit-learn's `FeatureUnion`.\n",
    "\n",
    "While scikit-learn pipelines help with managing the transformation from raw data, there may be many steps before this takes place in your pipeline. These pipelines are often referred to as ETL pipelines for (Extract, Transform, Load). In an ETL pipeline, the data is pulled or extracted from some source (like a database), transformed or manipulated, and then loaded into whatever system will analyze the data.\n",
    "\n",
    "Many data science teams rely on software tools to manage these ETL pipelines. If a transformation step fails, these tools alert you, or ensure that step can be re-run. If these transformation steps need to happen daily or weekly, these tools can manage that timeline.\n",
    "\n",
    "One of the most popular Python tools for this is [Luigi](https://github.com/spotify/luigi) developed by Spotify.\n",
    "An alternative is [Airflow](https://github.com/airbnb/airflow) by AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# test `make_pipeline` vs `Pipeline`; are they different?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "In pairs, assign one function to each pair, they have to read about it in the doc and then explain it to the class.\n",
    "\n",
    "1. Binarizer\n",
    "1. KernelCenterer\n",
    "1. MaxAbsScaler\n",
    "1. MinMaxScaler\n",
    "1. Normalizer\n",
    "1. OneHotEncoder\n",
    "1. PolynomialFeatures\n",
    "1. RobustScaler\n",
    "1. StandardScaler\n",
    "1. Data Imputation\n",
    "\n",
    "1. Imputer\n",
    "1. Function Transformer\n",
    "\n",
    "1. FunctionTransformer\n",
    "1. Label Manipulators\n",
    "\n",
    "1. LabelBinarizer\n",
    "1. LabelEncoder\n",
    "1. MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement custom transformers by extending the BaseClass in sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return X * self.factor\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "fm = FeatureMultiplier(2)\n",
    "\n",
    "test = np.diag((1,2,3,4))\n",
    "print test\n",
    "\n",
    "fm.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with `FunctionTransformer` from the preprocessing module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Implement a custom transformer that selects a specific feature from a Pandas dataframe. It should be initialized with the column name or the column index and it should return the selected column when transforming a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the salary prediction lab. How could you use `make_pipeline` and `make_union` to build a pipeline that performs the same steps all in one pass?\n",
    "\n",
    "You will have to build something like this:\n",
    "\n",
    ">Data: SelectCategoricalFeaturesTransformer: OneHotEncoder: FeatureUnion: Model: SelectNumericalFeaturesTransformer: Scaler\n",
    "\n",
    "Students:\n",
    "- Review lab and identify the steps that were performed\n",
    "- For each step, determine input and output\n",
    "- Is the input the whole dataframe or only a subset of the features?\n",
    "- Is the output new features or a prediction?\n",
    "- Identify what kind of transformer is needed:\n",
    "    - Is it a custom transformer?\n",
    "    - Does scikit-learn provide a transformer like this out of the box?\n",
    "- If features are treated differently, how do we recombine ([Feature Union](http://scikit-learn.org/stable/modules/pipeline.html)) them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
